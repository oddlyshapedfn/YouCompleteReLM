{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2X1LtZfOp9s"
   },
   "source": [
    "# How to use this notebook\n",
    "This notebook allows you to generate text with a language model on Google's\n",
    "spare resources, including older GPUs. GPU availability is not guaranteed, but\n",
    "this notebook works without it; it will just be slow. Please use these compute resources responsibly, as people who have better things to do than make fun of Pigroach also rely on them.\n",
    "\n",
    "To get started...\n",
    "1.   Click the little down arrow next to \"RAM\" and \"Disk\" in the top right,\n",
    "and click \"Change runtime type\" if electing to use a GPU. Select T4.\n",
    "2.   Click the same arrow and \"Connect to a hosted runtime\"\n",
    "3.   Run all the setup by clicking \"Runtime > Run All\" or the shortcut ctrl+F9\n",
    "4.   You can repeatedly generate new text by changing the settings in the last cell, and then rerunning it by clicking the Play arrow that appears when you mouse-over, or with ctrl+Enter.\n",
    "5.   When you're done, just close the tab. You'll need redo this process when you come back.\n",
    "\n",
    "The text used to train this model includes...\n",
    "*   Old SF google group\n",
    "*   Subset of SRK forums archive\n",
    "*   KoH forum up to ~mid 2023\n",
    "*   DSP's top-haters personal website\n",
    "*   Subset of discord leaks, including both WWE Champions and Mod discord.\n",
    "\n",
    "\n",
    "The training set does not include...\n",
    "*   Stream chat messages (too short)\n",
    "*   Tweets (usually too short)\n",
    "*   Any transcriptions of spoken word (stylistically different from written text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrqOMl1bG5Fn",
    "outputId": "00a8680c-abe7-4c63-d09d-538d5c7349c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: tqdm in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: ipywidgets in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (8.1.0)\n",
      "Requirement already satisfied: filelock in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipywidgets) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipywidgets) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: fsspec in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: backcall in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /home/kyle/micromamba/envs/llm/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tqdm ipywidgets\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPr6SkYFMm3z"
   },
   "source": [
    "# First do some setup behind the scenes...\n",
    "This cell downloads a pretrained model (~800MB for 410M, ~2.8GB for 1.4B) and configures it for text generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ghs1FytzG8bE",
    "outputId": "203a935d-51b8-43ba-89e9-2a3c345399eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers as tfs\n",
    "import torch\n",
    "\n",
    "class StopAtTok(tfs.StoppingCriteria):\n",
    "  def __init__(self, stoptok):\n",
    "    self.stoptok = stoptok[\"input_ids\"]\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n",
    "    return input_ids.flatten()[-1] == self.stoptok.flatten()\n",
    "\n",
    "class Inference():\n",
    "  def __init__(self, modelname, blocksize, device):\n",
    "    print(\"Implementing direct capture...\")\n",
    "    self.device = device\n",
    "    self.blocksize = blocksize\n",
    "    self.modelname = modelname\n",
    "    self.tokenizer = tfs.AutoTokenizer.from_pretrained(\n",
    "      modelname,\n",
    "    )\n",
    "\n",
    "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    print(\"Checking that the camera's not on...\")\n",
    "    self.config = tfs.AutoConfig.from_pretrained(modelname)\n",
    "    self.config.use_cache=True\n",
    "\n",
    "    print(\"Setting up Green Screen...\")\n",
    "    self.model = tfs.AutoModelForCausalLM.from_pretrained(\n",
    "      modelname,\n",
    "      config=self.config\n",
    "    ).to(self.device)\n",
    "    self.model.eval()\n",
    "    print(\"Loaded model.\")\n",
    "\n",
    "\n",
    "  def infer(self, opt):\n",
    "    prompt = opt['prompt']\n",
    "\n",
    "    inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True).to(self.device)\n",
    "    inputs = inputs.to(self.device)\n",
    "    prompt_len_tok = inputs['input_ids'].shape[-1]\n",
    "\n",
    "    if opt.get('length') is not None:\n",
    "        length = opt['length']\n",
    "    else:\n",
    "        length = self.blocksize\n",
    "\n",
    "    print(\"Prompt has size {}, leaving {} tokens for generation\".format(\n",
    "      prompt_len_tok,\n",
    "      length - prompt_len_tok\n",
    "    ))\n",
    "\n",
    "    generation_cfg = tfs.GenerationConfig(\n",
    "      do_sample=True,\n",
    "      eos_token_id=self.model.config.eos_token_id,\n",
    "      bos_token_id=self.model.config.bos_token_id,\n",
    "      pad_token_id=self.model.config.eos_token_id,\n",
    "      use_cache=True,\n",
    "      max_new_tokens=(length - prompt_len_tok),\n",
    "      temperature=opt['temperature'],\n",
    "      top_k=opt['top_k'],\n",
    "      top_p=opt['top_p'],\n",
    "      repetition_penalty=opt['repetition_penalty'],\n",
    "      length_penalty=1.0,\n",
    "      num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    stopper = StopAtTok(self.tokenizer(\"\\n\", return_tensors='pt').to(self.device))\n",
    "\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "      logits = self.model.generate(\n",
    "        **inputs,\n",
    "        generation_config=generation_cfg,\n",
    "        stopping_criteria=[stopper]\n",
    "      )\n",
    "      output = self.tokenizer.batch_decode(logits)\n",
    "\n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "id": "9OupzTThHKb6",
    "outputId": "38776bb1-7fec-4429-f790-c9f72ef90594"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing direct capture...\n",
      "Checking that the camera's not on...\n",
      "Setting up Green Screen...\n",
      "[2023-09-02 02:11:57,060] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Loaded model.\n"
     ]
    }
   ],
   "source": [
    "model_obj = Inference(\n",
    "  modelname = 'oddlyshapedfn/YouCompleteRe',\n",
    "  # modelname = 'ycr-chat',\n",
    "  blocksize = 512,\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36EZPZGRM0G5"
   },
   "source": [
    "# Use the cell below to generate text!\n",
    "Here are some prompt ideas. Try to capture the tone of the response you want but don't complete the sentence. You can change the prompt by replacing the text after \"prompt\": in the next cell. Include the `<YCR>:` tag in your prompt, it helps the model recognize that it should respond in the desired way.\n",
    "```\n",
    "<YCR>: I don't have time to explain how wrong you are, but\n",
    "<YCR>: This weekend's tournament was an utter disappointment because\n",
    "<YCR>: The thing my detractors don't get, and never will, is\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "A-kSWHqVHvqv",
    "outputId": "76e92c72-a81d-4778-8742-a73ed41e3f93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa95d79eb6bd497a95ef736278e2529e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='<YCR>:', description='prompt'), Output()), _dom_classes=('widget-interact',)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here are your knobs to influence the model output. Start with just the\n",
    "# prompt. The initial settings allow for a lot of randomness, so turn down\n",
    "# temperature and top_k if results become incoherent.\n",
    "#\n",
    "# After making your changes, rerun this cell to regenerate.\n",
    "opts = {\n",
    "    # Generate this many tokens.\n",
    "    \"length\": 200,\n",
    "    # Set higher for more randomness. Don't go too far over 1.0\n",
    "    \"temperature\": 0.9,\n",
    "    # Set higher for less repetitive responses. Stay < 1.5 to avoid nonsensical generations.\n",
    "    \"repetition_penalty\": 1.15,\n",
    "    # Set higher for more random responses.\n",
    "    \"top_k\": 75,\n",
    "    # Set higher for more random responses. Value should be between 0 and 1.\n",
    "    \"top_p\": 0.65,\n",
    "    # The beginning of the text to complete. Start with \"<YCR>:\" for best results\n",
    "    \"prompt\": \"<YCR>\"\n",
    "}\n",
    "@widgets.interact(prompt=\"<YCR>:\")\n",
    "def f(prompt):\n",
    "    newopts = opts.copy()\n",
    "    newopts['prompt'] = prompt\n",
    "    print(\"INPUT: {}\".format(prompt))\n",
    "    print(\"=========================\")\n",
    "    response = model_obj.infer(newopts)\n",
    "    print(\"RESPONSE: {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
